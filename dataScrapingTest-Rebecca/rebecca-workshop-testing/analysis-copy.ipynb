{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('test.json', encoding='utf-8')\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "output = data\n",
    "##People create empty columns\n",
    "#Wagner, Kant, Schopenhauer\n",
    "output['matches_schop'] = ''\n",
    "output['matches_wagn'] = ''\n",
    "output['matches_kant'] = ''\n",
    "\n",
    "##General\n",
    "#Deutschland, musik\n",
    "output['matches_deu'] = ''\n",
    "output['matches_musik'] = ''\n",
    "\n",
    "##Philosophy terms\n",
    "#Wille, Ding an sich, Wahrheit, Tragische/Tragodie, Leiden\n",
    "output['matches_wille'] = ''\n",
    "output['matches_dingansich'] = ''\n",
    "output['matches_wahrheit'] = ''\n",
    "output['matches_trag'] = ''\n",
    "output['matches_leiden'] = ''\n",
    "\n",
    "##Religion\n",
    "#Hindusimus, Buddhismus, Schleier de Maja,\n",
    "output['matches_hinduism'] = '' #problems with finding matches\n",
    "output['matches_buddhism'] = ''\n",
    "output['matches_shcleier'] = '' #problems with finding matches\n",
    "\n",
    "for i, row in output.iterrows():\n",
    "    chunk = row['text']\n",
    "    chunk = str(chunk)\n",
    "\n",
    "    #get entire length of fragment\n",
    "    pAll=re.compile(r'.*')\n",
    "    iterAll = pAll.finditer(chunk)\n",
    "    for match in iterAll:\n",
    "        span = match.start()\n",
    "    output.at[i, 'length'] = span\n",
    "\n",
    "    ##People start counting\n",
    "\n",
    "    #schopenhauer\n",
    "    pSchop = re.compile(r'\\b[Ss]chopenhauer\\w+') # will match all words beginning with 'Schopenhauer' or 'schopenhauer'\n",
    "    output.at[i, 'matches_schop'] = pSchop.findall(chunk)\n",
    "    output.at[i, 'num_schop'] = len(pSchop.findall(chunk))\n",
    "\n",
    "    #wagner\n",
    "    pWagner = re.compile(r'\\b[Ww]agner\\w+')\n",
    "    output.at[i, 'matches_wagn'] = pWagner.findall(chunk)\n",
    "    output.at[i, 'num_wagn'] = len(pWagner.findall(chunk))\n",
    "\n",
    "    #kant\n",
    "    pKant = re.compile(r'\\b[Kk]ant\\w+')\n",
    "    output.at[i, 'matches_kant'] = pKant.findall(chunk)\n",
    "    output.at[i, 'num_kant'] = len(pKant.findall(chunk))\n",
    "\n",
    "    ##General start counting\n",
    "\n",
    "    #Deutschland\n",
    "    pDeutsch = re.compile(r'\\b[Dd]eutsch\\w+')\n",
    "    output.at[i, 'matches_deu'] = pDeutsch.findall(chunk)\n",
    "    output.at[i, 'num_deu'] = len(pDeutsch.findall(chunk))\n",
    "\n",
    "    #musik\n",
    "    pMusik = re.compile(r'\\b[Mm]usik\\w+')\n",
    "    output.at[i, 'matches_musik'] = pMusik.findall(chunk)\n",
    "    output.at[i, 'num_musik'] = len(pMusik.findall(chunk))\n",
    "\n",
    "    ##Philosophy terms start counting\n",
    "\n",
    "    #wille\n",
    "    pWille = re.compile(r'\\b [Ww]ille\\w+')\n",
    "    output.at[i, 'matches_wille'] = pWille.findall(chunk)\n",
    "    output.at[i, 'num_wille'] = len(pWille.findall(chunk))\n",
    "\n",
    "    #ding an sich\n",
    "    # phrase = '„Ding an sich“'\n",
    "    pDingansich = re.compile(r'\\bDing an sich\\b')\n",
    "    output.at[i, 'matches_dingansich'] = pDingansich.findall(chunk)\n",
    "    output.at[i, 'num_dingansich'] = len(pDingansich.findall(chunk))\n",
    "    \n",
    "    #wahrheit\n",
    "    pWahrheit = re.compile(r'\\b [Ww]ahrheit\\w+')\n",
    "    output.at[i, 'matches_wahrheit'] = pWahrheit.findall(chunk)\n",
    "    output.at[i, 'num_wahrheit'] = len(pWahrheit.findall(chunk))\n",
    "\n",
    "    #tragische\n",
    "    pTragische = re.compile(r'\\b [Tt]ragi\\w+')\n",
    "    output.at[i, 'matches_trag'] = pTragische.findall(chunk)\n",
    "    output.at[i, 'num_trag'] = len(pTragische.findall(chunk))\n",
    "\n",
    "    #leiden\n",
    "    pLeiden = re.compile(r'\\b [Ll]eiden\\w+')\n",
    "    output.at[i, 'matches_leiden'] = pLeiden.findall(chunk)\n",
    "    output.at[i, 'num_leiden'] = len(pLeiden.findall(chunk))\n",
    "\n",
    "    ##Religion \n",
    "\n",
    "    #Hindusimus ** Problems finding matches **\n",
    "    pHindu = re.compile(r'\\b Hindus\\b')\n",
    "    pVedanta = re.compile(r'\\b [Vv]eda\\w+')\n",
    "    output.at[i, 'matches_hinduism'] = pHindu.findall(chunk) + pVedanta.findall(chunk)\n",
    "    output.at[i, 'num_hinduism'] = len(pHindu.findall(chunk)) + len(pVedanta.findall(chunk))\n",
    "\n",
    "    #Buddhismus \n",
    "    pBuddhism = re.compile(r'\\b [Bb]uddhis\\w+')\n",
    "    output.at[i, 'matches_buddhism'] = pBuddhism.findall(chunk)\n",
    "    output.at[i, 'num_buddhism'] = len(pBuddhism.findall(chunk))\n",
    "\n",
    "    #Schleier\n",
    "    pSchleier = re.compile(r'\\bSchleier\\b')\n",
    "    output.at[i, 'matches_shcleier'] = pSchleier.findall(chunk)\n",
    "    output.at[i, 'num_shcleier'] = len(pSchleier.findall(chunk))\n",
    "\n",
    "\n",
    "output.to_csv('analysis4.csv')\n",
    "output['num_shcleier'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_json(r'data.json', orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reformatting so that svelte app can more easily work with data\n",
    "array = data\n",
    "array['matches_schop'] = ''\n",
    "array['pos_schop'] = ''\n",
    "array['frac_schop'] = ''\n",
    "\n",
    "for i, row in array.iterrows():\n",
    "    chunk = row['text']\n",
    "    chunk = str(chunk)\n",
    "\n",
    "    pAll=re.compile(r'.*')\n",
    "    iterAll = pAll.finditer(chunk)\n",
    "    for match in iterAll:\n",
    "        span = match.start()\n",
    "    text_length = span\n",
    "\n",
    "    pSchop = re.compile(r'\\b[Ss]chopenhauer\\w+') # will match all words beginning with 'Schopenhauer' or 'schopenhauer'\n",
    "    matches_schop = pSchop.findall(chunk)\n",
    "    num_matches = len(pSchop.findall(chunk))\n",
    "    \n",
    "    positions = []  \n",
    "    fractions = []\n",
    "    iterSchop = pSchop.finditer(chunk)\n",
    "    for match in iterSchop:\n",
    "        newPosition = match.start()\n",
    "        newFraction = newPosition/span\n",
    "        positions.append(newPosition)\n",
    "        fractions.append(newFraction)\n",
    "\n",
    "    if num_matches = 0:\n",
    "\n",
    "    else:\n",
    "        for i in num_matches:\n",
    "            j = {\n",
    "                'year': int(row['year']),\n",
    "                'group': int(row['group']),\n",
    "                'text': row['text'],\n",
    "                'length': text_length,\n",
    "                'schop_matches': {\n",
    "                    'num_matches': num_matches,\n",
    "                    'matched_text': matches_schop,\n",
    "                    'matched_positions': positions,\n",
    "                    'matched_fractions': fractions,\n",
    "                    'length': text_length\n",
    "                }\n",
    "    }\n",
    "\n",
    "    with open('data_jsonified.json') as file:\n",
    "        a = json.load(file)\n",
    "        a.append(j)\n",
    "    with open('data_jsonified.json', 'w') as file:\n",
    "        json.dump(a, file, indent=4)\n",
    "\n",
    "    j = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}